import os
import requests
import pandas as pd
import time
import re
import streamlit as st
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
exchange_key = os.getenv("EXCHANGE_RATE_KEY")

# ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ë¸Œë¼ìš°ì € í—¤ë” ì„¤ì •
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.kita.net/board/totalTradeNews/totalTradeNewsList.do"
}

def get_exchange_rate():
    try:
        url = f"https://v6.exchangerate-api.com/v6/{exchange_key}/latest/USD"
        response = requests.get(url, timeout=5)
        return response.json()['conversion_rates']['KRW']
    except:
        return 1431.35 # ì˜¤ë¥˜ ì‹œ ì´ë¯¸ì§€ìƒì˜ ê¸°ë³¸ê°’ í‘œì‹œ

def get_full_trade_news(max_pages=5):
    data = []
    keywords = ["ì¤‘êµ­", "íŠ¸ëŸ¼í”„"] 
    session = requests.Session() # ì„¸ì…˜ ìœ ì§€ë¥¼ í†µí•´ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ì„± ê°•í™”
    
    for page in range(1, max_pages + 1):
        list_url = f"https://www.kita.net/board/totalTradeNews/totalTradeNewsList.do?pageIndex={page}"
        try:
            response = session.get(list_url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, "html.parser")
            
            # [ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼] .board-list > li êµ¬ì¡° ì‚¬ìš©
            items = soup.select(".board-list > li")
            
            if not items:
                continue

            for item in items:
                title_tag = item.select_one(".subject a")
                if not title_tag: continue
                
                title = title_tag.text.strip()
                
                # í‚¤ì›Œë“œ í•„í„°ë§
                if any(key in title for key in keywords):
                    # onclick ì†ì„±ì—ì„œ noì™€ siteId ì¶”ì¶œ (ë³¸ë¬¸ ìˆ˜ì§‘ì˜ í•µì‹¬)
                    onclick = title_tag.attrs.get('onclick', '')
                    params = re.findall(r"'(.*?)'", onclick)
                    
                    if len(params) >= 2:
                        news_no, site_id = params[0], params[1]
                        link = f"https://www.kita.net/board/totalTradeNews/totalTradeNewsDetail.do?no={news_no}&siteId={site_id}"
                        
                        # ìƒì„¸ í˜ì´ì§€ ë³¸ë¬¸ í¬ë¡¤ë§
                        detail_res = session.get(link, headers=headers, timeout=10)
                        detail_soup = BeautifulSoup(detail_res.text, "html.parser")
                        
                        # [ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼] ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ
                        content_tag = detail_soup.select_one(".boardView_cont")
                        if not content_tag:
                            content_tag = detail_soup.select_one(".view_cont")
                            
                        content = content_tag.get_text(separator="\n").strip() if content_tag else "ë³¸ë¬¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        
                        date_tag = item.select_one(".info")
                        date = date_tag.text.strip() if date_tag else "ë‚ ì§œì—†ìŒ"
                        
                        # KeyError ë°©ì§€ë¥¼ ìœ„í•´ ë”•ì…”ë„ˆë¦¬ í‚¤ ì´ë¦„ í†µì¼
                        data.append({
                            "ë‚ ì§œ": date, 
                            "ì œëª©": title, 
                            "ë³¸ë¬¸": content, 
                            "link": link 
                        })
                        time.sleep(0.4)
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
            
    return data

# --- Streamlit UI êµ¬ì„± ---
st.set_page_config(page_title="ì¤‘êµ­/íŠ¸ëŸ¼í”„ ë¬´ì—­ ëª¨ë‹ˆí„°ë§", layout="wide")
st.title("ğŸš¢ í•µì‹¬ ë¬´ì—­ ì´ìŠˆ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ")

# í™˜ìœ¨ í‘œì‹œ
krw_rate = get_exchange_rate()
st.metric(label="í˜„ì¬ ì›/ë‹¬ëŸ¬ í™˜ìœ¨ (USD/KRW)", value=f"{krw_rate:,.2f} ì›")

st.divider()

# ê²€ìƒ‰ í˜ì´ì§€ ì„¤ì •
target_pages = st.sidebar.number_input("ê²€ìƒ‰í•  í˜ì´ì§€ ìˆ˜", 1, 10, 5)

if st.button("ì´ìŠˆ ìˆ˜ì§‘ ë° ë³¸ë¬¸ ë¶„ì„ ì‹œì‘"):
    with st.spinner('KITA ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
        news_results = get_full_trade_news(max_pages=target_pages)
        
        if news_results:
            st.success(f"âœ… ì´ {len(news_results)}ê±´ì˜ í•µì‹¬ ì´ìŠˆë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
            
            # [í•µì‹¬ ê¸°ëŠ¥] ì œëª© í´ë¦­ ì‹œ ë³¸ë¬¸ ë‚´ìš© í¼ì¹˜ê¸°
            for news in news_results:
                with st.expander(f"[{news['ë‚ ì§œ']}] {news['ì œëª©']}"):
                    st.write(f"**ğŸ”— ì›ë¬¸ ë§í¬:** [ë°”ë¡œê°€ê¸°]({news['link']})")
                    st.markdown("---")
                    st.write(news['ë³¸ë¬¸'])
            
            # ì—‘ì…€ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ
            df = pd.DataFrame(news_results)
            try:
                df.to_excel("ë¬´ì—­_í•µì‹¬ì´ìŠˆ_ë¦¬í¬íŠ¸.xlsx", index=False)
                st.info("ğŸ“‚ 'ë¬´ì—­_í•µì‹¬ì´ìŠˆ_ë¦¬í¬íŠ¸.xlsx' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except PermissionError:
                st.error("âš ï¸ ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë‹«ê³  ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        else:
            st.warning("ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")